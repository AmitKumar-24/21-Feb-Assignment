{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f1b818-4066-4a45-af59-12b170c7b31b",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7979b4-176f-4459-8046-58e3e9c9f3dd",
   "metadata": {},
   "source": [
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
    "\n",
    "There are many areas where Web Scrapping is used to get data.\n",
    "\n",
    "1. Price Monitoring : Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
    "\n",
    "2. Market Research : Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. \n",
    "\n",
    "3. News Monitoring : Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!\n",
    "\n",
    "4. Email Marketing : Companies can also use Web scraping for email marketing. They can collect Email ID’s from various sites using web scraping and then send bulk promotional and marketing Emails to all the people owning these Email ID’s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d0bd6-abf4-4457-8bb5-df7bb285109f",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315beb0-3037-4853-8a6b-4c70a823a2f4",
   "metadata": {},
   "source": [
    "There are various methods used for Web Scraping are : \n",
    "\n",
    "1.Browser extensions Web Scrapers are extensions that can be added to your browser. These are easy to run as they are integrated with your browser, but at the same time, they are also limited because of this. Any advanced features that are outside the scope of your browser are impossible to run on Browser extension Web Scrapers.\n",
    "\n",
    "2.Software Web Scrapers don’t have these limitations as they can be downloaded and installed on your computer. These are more complex than Browser web scrapers, but they also have advanced features that are not limited by the scope of your browser.\n",
    "\n",
    "3.Cloud Web Scrapers run on the cloud, which is an off-site server mostly provided by the company that you buy the scraper from. These allow your computer to focus on other tasks as the computer resources are not required to scrape data from websites. \n",
    "\n",
    "4.Local Web Scrapers, on the other hand, run on your computer using local resources. So, if the Web scrapers require more CPU or RAM, then your computer will become slow and not be able to perform other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b331c-c171-405a-b1a0-d545e8df1c15",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3c542-f99c-45ed-a2ca-e464678545aa",
   "metadata": {},
   "source": [
    "Beautiful Soup provides simple methods for navigating, searching, and modifying a parse tree in HTML, XML files. It transforms a complex HTML document into a tree of Python objects. It also automatically converts the document to Unicode, so you don’t have to think about encodings. This tool not only helps you scrape but also to clean the data. Beautiful Soup supports the HTML parser included in Python’s standard library, but it also supports several third-party Python parsers like lxml or hml5lib. It creates a parse tree for parsed pages that can be used to extract data from HTML which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b52121-e9a7-42a7-8a42-c05995adcd7f",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab222a-418a-42f8-8af5-c0f07fcd0d6c",
   "metadata": {},
   "source": [
    "Flask is a lightweight framework to build websites. We’ll use this to parse our collected data and display it as HTML in a new HTML file.\n",
    "The requests module allows us to send http requests to the website we want to scrape.\n",
    "\n",
    "In our file, type the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebfe3f7-6b0e-44cb-b48f-61596772db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template,\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe73d24-2c20-4e25-9096-d28071c4c7af",
   "metadata": {},
   "source": [
    "The first line imports the Flask class and the render_template method from the flask library. The second line imports the BeautifulSoup class, and the third line imports the requests module from our Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68df48-c407-4ad0-98b9-170af5b62ab2",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0d955-d7b9-4fb4-aabc-4dce43872741",
   "metadata": {},
   "source": [
    "AWS services used in this project :\n",
    "\n",
    "1.AWS Elastic Beanstalk : is an AWS managed service for web applications. Elastic beanstalk is a pre-confiured EC2 server that can directly take up your application code and environment configuration and use it to automatically provision and deploy the required resources within AWS to run the web application. \n",
    "\n",
    "AWS Elastic Beanstalk Features:\n",
    "\n",
    "Application: Elastic Beanstalk directly takes in out project code. So Elastic Beanstalk application is named the same as your project home directory.\n",
    "\n",
    "Application Environments: Users may want their application to run on different environments like DEV, UAT and PROD. You can create and configure different environments to run application on different stages.\n",
    "\n",
    "Environment Health:  One of the most lucrative features about running application on AWS or most of the other cloud platforms is the automated health checks.\n",
    "\n",
    "Scalability: Using Auto-Scaling within Elastic beanstalk makes the application dynamically scalable.\n",
    "\n",
    "Language support: Elastic Beanstalk supports the applications developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\n",
    "\n",
    "Pricing: There is no extra charge for using Elastic Beanstalk. Users ar only required to pay for the services and resources provisioned by Elastic Beanstalk Service.\n",
    "\n",
    "2.AWS CodePipeline : is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285a236-5922-4143-9a54-5510c25e5aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
